# Performance Testing Report: Microservices Scale-Out Architecture
## 10Ã— Repetition Testing (Baseline & Stress Scenarios)

**Report Generated:** January 1, 2026  
**Analysis Tool:** k6 v0.48+  
**Data Processing:** PowerShell 7+ with JSON parsing  
**Report Version:** 1.1 (Updated: toned down claims, clarified scope, added baseline throughput analysis)

---

## Executive Summary

This report presents comprehensive performance analysis of the **Microservices Scale-Out Architecture** under baseline (35 VUs) and stress (75 VUs) conditions. Each scenario was executed **10 times** to establish statistical reliability and assess performance consistency with horizontal scaling.

**Key Results:**
- âœ… **Outstanding scaling efficiency:** 112% throughput scaling (2.40Ã—) for 2.14Ã— load increase
- âœ… **Sub-linear latency degradation:** 1.38Ã— p95 increase under 2.14Ã— load
- âœ… **100% reliability:** Zero error rate across all 20 test runs
- âœ… **Excellent consistency:** CV < 14% across all metrics
- âœ… **Superior to single-node:** 2.4Ã— throughput vs 2.2Ã— (single-node microservices)

---

## 1. Test Methodology

### 1.1 Baseline Scenario
- **Virtual Users (VUs):** 35
- **Duration:** 10 minutes
- **Ramp Pattern:**
  - 30 seconds: ramp up 0 â†’ 25 VUs (stage 1)
  - 0 seconds: immediate jump 25 â†’ 35 VUs (stage 2 start)
  - 9 minutes: sustain 35 VUs (stage 2)
  - 30 seconds: ramp down 35 â†’ 0 VUs (stage 3)
- **Purpose:** Establish performance baseline with scaled infrastructure

### 1.2 Stress Scenario
- **Virtual Users (VUs):** 75
- **Duration:** 8 minutes
- **Ramp Pattern:**
  - 30 seconds: ramp up 0 â†’ 25 VUs (stage 1)
  - 0 seconds: immediate jump 25 â†’ 75 VUs (stage 2 start)
  - 6 minutes 30 seconds: sustain 75 VUs (stage 2)
  - 30 seconds: ramp down 75 â†’ 0 VUs (stage 3)
- **Purpose:** Measure system performance under high load with scaled services

### 1.3 Service Level Objectives (SLOs)
- **p95 latency:** < 2,800 ms (95th percentile response time)
- **p99 latency:** < 5,500 ms (99th percentile response time)
- **Error rate:** < 0.1% (http_req_failed)

### 1.4 Test Environment
- **Architecture:** Microservices with Docker Compose scale-out
- **Service Scaling Configuration:**
  - **layanan-api-gateway:** 2 replicas
  - **layanan-manajemen-pengguna:** 2 replicas
  - **layanan-pendaftaran:** 3 replicas
  - **layanan-alur-kerja:** 2 replicas
  - **layanan-survei:** 2 replicas
  - **layanan-arsip:** 2 replicas
- **Load Balancer:** Nginx (reverse proxy with round-robin)
- **Databases:** 
  - MySQL per service (jelita-user-mysql, jelita-survey-mysql, etc.)
  - Redis (shared cache)
- **Base URL:** http://localhost:8080 (via Nginx)
- **Infrastructure:** Docker containers on single machine
- **Host Specifications:**
  - **OS:** Windows 11 Pro (Build 22000+)
  - **CPU:** Multi-core processor supporting concurrent container execution
  - **RAM:** 16GB+ DDR4
  - **Docker:** Docker Desktop 4.x+ (Linux containers)
  - **Node.js:** v18.x LTS (per service container)
  - **MySQL:** 8.0.x (per service container)
  - **Redis:** 7.x (shared container)
  - **Nginx:** Latest stable (gateway container)
  - **k6:** v0.48+ (running on same host)
- **Container Configuration:**
  - No explicit CPU/memory limits set (uses Docker Desktop defaults)
  - Multiple service replicas per layer
  - Each service container: independent Node.js process
- **Network:** Docker bridge network with Nginx routing
- **Note:** Scale-out within single host (vertical scaling of replicas, not distributed multi-host)

### 1.5 Test Execution Details
- **Test Runs:** 10 repetitions per scenario (baseline & stress)
- **Total Tests:** 20 test executions
- **Test Dates:** December 31, 2025 (all 10 runs: r01-r10)
- **Data Collection:** 
  - summary-export.json (complete metrics in JSON format)
  - summary.txt (quick reference text summary)
  - metrics.csv (raw time-series data)

---

## 2. Test Results

### 2.1 Baseline Performance (35 VUs)

#### HTTP Request Duration Statistics (10 runs)
| Metric | Mean Â± StdDev | 95% CI | Min | Max | CV (%) |
|--------|---------------|--------|-----|-----|--------|
| **Throughput** | **21.88 Â± 0.38 req/s** | **[21.61, 22.15]** | 21.25 | 22.54 | **1.7%** |
| **p95** | **606.93 Â± 50.25 ms** | **[570.98, 642.87]** | 519.58 ms | 678.97 ms | **8.3%** |
| **p99** | **830.69 Â± 59.47 ms** | **[788.15, 873.23]** | 727.02 ms | 910.54 ms | **7.2%** |

**Analysis:**
- âœ… **Excellent consistency:** CV = 8.3% for p95, 7.2% for p99 (high consistency)
- âœ… **All thresholds met:** p95 (607 ms) < 2.8s, p99 (831 ms) < 5.5s
- âœ… **Zero failures:** 0% error rate across all 10 runs
- âœ… **Very stable throughput:** 21.88 Â± 0.38 req/s (CV = 1.7%, exceptional consistency)
- ðŸ“Š **95% CI interpretation:**
  - Throughput: [21.61, 22.15] - very narrow range, highly reliable estimate
  - p95 latency: [570.98, 642.87] - tight CI indicates consistent scaled performance
  - With 95% confidence, true mean falls within these intervals

#### Detailed Performance Metrics (Baseline)
| Run | Throughput (req/s) | p95 (ms) | p99 (ms) |
|-----|-------------------|----------|----------|
| r01 | 22.04 | 624.44 | 876.74 |
| r02 | 21.80 | 638.51 | 877.72 |
| r03 | 22.43 | 519.58 | 755.67 |
| r04 | 21.68 | 605.36 | 781.50 |
| r05 | 21.93 | 601.63 | 812.15 |
| r06 | 21.75 | 629.47 | 847.08 |
| r07 | 21.74 | 629.75 | 865.80 |
| r08 | 21.25 | 678.97 | 910.54 |
| r09 | 21.68 | 620.43 | 852.71 |
| r10 | 22.54 | 521.12 | 727.02 |

---

### 2.2 Stress Performance (75 VUs)

#### HTTP Request Duration Statistics (10 runs)
| Metric | Mean Â± StdDev | 95% CI | Min | Max | CV (%) |
|--------|---------------|--------|-----|-----|--------|
| **Throughput** | **52.60 Â± 1.06 req/s** | **[51.84, 53.35]** | 51.11 | 54.68 | **2.0%** |
| **p95** | **839.94 Â± 70.76 ms** | **[789.32, 890.55]** | 753.91 ms | 981.77 ms | **8.4%** |
| **p99** | **1,268.38 Â± 167.55 ms** | **[1,148.52, 1,388.23]** | 1,101.99 ms | 1,564.09 ms | **13.2%** |

**Analysis:**
- âœ… **Outstanding stability:** CV = 8.4% for p95, 13.2% for p99 (good consistency under stress)
- âœ… **All thresholds met:** p95 (840 ms) < 2.8s, p99 (1,268 ms) < 5.5s
- âœ… **Zero failures:** 0% error rate across all runs
- âœ… **High throughput:** 52.60 Â± 1.06 req/s (CV = 2.0%, excellent consistency)
- âœ… **Throughput scaling:** +140% vs baseline (52.60 vs 21.88 req/s)
- ðŸ“Š **95% CI interpretation:**
  - Throughput: [51.84, 53.35] - narrow range despite high load
  - p95 latency: [789.32, 890.55] - reasonable CI width under stress
  - Similar CV to baseline indicates predictable scaling behavior

#### Detailed Performance Metrics (Stress)
| Run | Throughput (req/s) | p95 (ms) | p99 (ms) |
|-----|-------------------|----------|----------|
| r01 | 51.56 | 878.46 | 1,263.53 |
| r02 | 52.93 | 808.97 | 1,143.41 |
| r03 | 52.35 | 814.71 | 1,191.44 |
| r04 | 51.70 | 922.91 | 1,524.07 |
| r05 | 53.35 | 782.50 | 1,156.25 |
| r06 | 53.35 | 776.09 | 1,101.99 |
| r07 | 54.68 | 753.91 | 1,134.45 |
| r08 | 52.07 | 850.21 | 1,396.60 |
| r09 | 52.87 | 829.82 | 1,207.94 |
| r10 | 51.11 | 981.77 | 1,564.09 |

---

## 3. Horizontal Scaling Analysis

### 3.1 Scale-Out Benefits vs Single-Node Microservices

**Comparison with Single-Node Microservices (from previous report):**

| Metric | Single-Node | Scale-Out | Improvement |
|--------|-------------|-----------|-------------|
| **Baseline p95** | 610.89 ms | 606.93 ms | **0.6% faster** |
| **Stress p95** | 2,437.30 ms | 839.94 ms | **65.5% faster** |
| **Stress Throughput** | 37.08 req/s | 52.60 req/s | **+41.8% higher** |
| **Stress Scaling** | 2.22Ã— (104% eff.) | 2.40Ã— (112% eff.) | **+8% better** |

**Key Insights:**
- âœ… **Dramatic stress improvement:** Scale-out reduces p95 latency by 65.5% under high load
- âœ… **Higher throughput capacity:** +41.8% more requests handled vs single-node
- âœ… **Better scaling efficiency:** 112% efficiency (super-linear) vs 104% (single-node)
- âœ… **Load distribution advantage:** Multiple replicas prevent bottlenecks at individual services
- âš ï¸ **Baseline parity:** Similar baseline performance (scaling benefits emerge under load)

### 3.2 Resource Distribution Hypothesis

**Why Scale-Out Shows Super-Linear Efficiency (112%):**

1. **Load Balancing Effect:**
   - Nginx distributes requests across replicas
   - Prevents single-service CPU saturation
   - Reduces queueing delays at service level

2. **Parallel Processing:**
   - 2-3 replicas per service = more concurrent request handling
   - Independent Node.js event loops per replica
   - Database query distribution across connections

3. **Reduced Contention:**
   - Each replica has dedicated resources (within single host)
   - Less mutex contention in Node.js process
   - Better CPU cache utilization per smaller process

4. **Network Overhead Trade-off:**
   - Additional Nginx hop adds minimal latency (~5-10ms)
   - Offset by significantly reduced service-level queueing
   - Net benefit increases with load

**Caveat:** Super-linear scaling (>100%) on single host suggests that single-node microservices were resource-constrained. Scale-out alleviates this bottleneck by distributing load across replicas.

---

## 4. Performance Comparison: Baseline vs Stress

| Metric | Baseline (35 VUs) | Stress (75 VUs) | Degradation Factor |
|--------|-------------------|-----------------|-------------------|
| **VUs** | 35 | 75 | **2.14Ã—** |
| **HTTP p95** | 606.93 ms | 839.94 ms | **1.38Ã—** |
| **HTTP p99** | 830.69 ms | 1,268.38 ms | **1.53Ã—** |
| **Throughput** | 21.88 req/s | 52.60 req/s | **2.40Ã—** |
| **Error Rate** | 0% | 0% | **No change** |

**Key Insight:**
- âœ… **Excellent sub-linear degradation:** 2.14Ã— load â†’ 1.38Ã— latency (64% efficiency)
- âœ… **Super-linear throughput scaling:** 2.40Ã— throughput for 2.14Ã— VUs (112% efficiency)
- âœ… **Horizontal scaling advantage:** Better than monolith (1.80Ã—) and single-node microservices (2.22Ã—)
- âœ… System demonstrates excellent capacity under scaled-out configuration

**Baseline Throughput Observation:**
- Scale-out baseline (21.88 req/s) is lower than single-node microservices baseline (27.58 req/s)
- Possible causes (not isolated in this study):
  - Additional Nginx routing hop adds latency overhead at low loads
  - More containers (13 vs 8) increase I/O and CPU contention on single host during initialization
  - Different database initialization states between test runs
- **Key finding:** Scale-out benefits emerge under stress (52.60 vs 37.62 req/s), validating horizontal scaling value

---

## 5. Reliability Assessment

| Metric | Baseline (35 VUs) | Stress (75 VUs) | Total |
|--------|-------------------|-----------------|-------|
| **Total Tests** | 10 | 10 | 20 |
| **Passed Tests** | 10 | 10 | 20 |
| **Failed Tests** | 0 | 0 | 0 |
| **Success Rate** | **100%** | **100%** | **100%** |
| **Error Rate** | **0%** | **0%** | **0%** |

---

## 6. Key Findings

### 6.1 Performance Characteristics

1. **Outstanding Latency Performance:**
   - Baseline p95: 607 ms with good variation (Â±50 ms, CV 8.3%)
   - Stress p95: 840 ms with acceptable variation (Â±71 ms, CV 8.4%)
   - Both well within SLO thresholds (<2.8s)
   - **Major advantage:** Stress latency 65.5% lower than single-node microservices

2. **Super-Linear Scalability:**
   - 2.14Ã— load increase â†’ 1.38Ã— latency increase (64% efficiency)
   - Throughput scales super-linearly: 2.40Ã— increase for 2.14Ã— VUs (112% efficiency)
   - Demonstrates excellent horizontal scaling within single host

3. **Zero Error Rate:**
   - 100% success rate across all 20 tests
   - No failures, timeouts, or errors observed

4. **Consistent Performance:**
   - CV < 14% across all metrics
   - Predictable and reliable under varied conditions

### 6.2 Strengths of Scale-Out Architecture

âœ… **Load Distribution:** Multiple replicas prevent single-service bottlenecks  
âœ… **High Throughput:** 52.60 req/s under stress (41.8% more than single-node)  
âœ… **Sub-Linear Degradation:** Latency grows slower than load  
âœ… **Improved Availability:** Multiple replicas improve availability against single-container failure (not full HA; still single-host)  
âœ… **Horizontal Scalability:** Proven ability to scale within resource limits

### 6.3 Performance Stability

- **Baseline:** Suitable for sustained 35 VUs with ~607ms p95 latency
- **Stress:** Handles 75 VUs with <840ms p95 latency (excellent)
- **Upper Limit:** Not determined in this study; future work should test 100-150 VUs to find saturation point

---

## 7. Conclusions

1. **Reliability Excellence:**
   - âœ… 100% test success rate (20/20 tests passed)
   - âœ… Zero error rate across all scenarios
   - âœ… All SLO thresholds consistently met

2. **Performance Consistency:**
   - âœ… Excellent CV values (2-13%)
   - âœ… Predictable latency patterns
   - âœ… Minimal variation between runs

3. **Scalability:**
   - âœ… Sub-linear latency degradation under load
   - âœ… Super-linear throughput scaling (112% efficiency)
   - âœ… Superior to both monolith and single-node microservices under stress

4. **Production Readiness:**
   - âœ… Scale-out architecture is **promising under tested workload** in single-host environment
   - âœ… Demonstrated stable performance for 35-75 concurrent users in controlled testing
   - âœ… Shows predictable behavior with horizontal scaling benefits
   - âš ï¸ **Further validation needed:** Multi-host production deployment, resource monitoring (CPU/mem), isolated load testing
   - âš ï¸ Results specific to single-host scale-out without explicit resource limits; production deployment requires additional tuning

---

## 8. Recommendations

1. **Deployment Strategy:**
   - âœ… **Promising approach** for workloads requiring high stress performance
   - Scale-out provides significant latency advantage (65.5% faster) under high load in tested conditions
   - Lower stress latency than monolith (840ms vs 1,684ms)
   - âš ï¸ Requires further validation: multi-host deployment, production-grade monitoring, resource limits configuration

2. **Capacity Planning:**
   - Current capacity: comfortable up to 75 VUs with excellent latency
   - Recommend load testing at 100-150 VUs to find upper limits
   - Consider further horizontal scaling (more replicas) or multi-host distribution

3. **Monitoring Priorities:**
   - Track p95/p99 latency trends per service
   - Monitor replica load distribution (ensure balanced traffic)
   - Track Nginx connection pool and routing efficiency
   - Alert on error rate > 0.1%

4. **Architecture Decision Matrix:**
   - **Choose Scale-Out Microservices if:**
     - High concurrency expected (>50 concurrent users)
     - Sub-second latency required under stress
     - Need improved availability against single-container failure (within single host)
     - Have adequate host resources (RAM, CPU for multiple containers)
   - **Choose Single-Node Microservices if:**
     - Lower concurrency (<35 concurrent users)
     - Resource-constrained environment
     - Simpler deployment preferred
   - **Choose Monolith if:**
     - Low concurrency (<35 concurrent users)
     - Simplest operations required
     - Lower baseline latency critical

---

## 9. Architecture Comparison Summary

### Performance Comparison Across All Architectures

| Architecture | Baseline p95 | Stress p95 | Stress Throughput | Scaling Efficiency |
|--------------|--------------|------------|-------------------|-------------------|
| **Monolith** | 1,026.51 ms | 1,683.78 ms | 36.08 req/s | 84% (near-linear) |
| **Microservices (single-node)** | 610.89 ms | 2,437.30 ms | 37.08 req/s | 104% (linear+) |
| **Microservices (scale-out)** | 606.93 ms | **839.94 ms** | **52.60 req/s** | **112% (super-linear)** |

**Winner by Category:**
- ðŸ¥‡ **Best Baseline Latency:** Microservices Scale-Out (606.93 ms)
- ðŸ¥‡ **Best Stress Latency:** Microservices Scale-Out (839.94 ms) - 50% faster than monolith, 65% faster than single-node
- ðŸ¥‡ **Best Stress Throughput:** Microservices Scale-Out (52.60 req/s) - 46% higher than monolith
- ðŸ¥‡ **Best Scaling Efficiency:** Microservices Scale-Out (112%)

**Trade-offs:**
- **Scale-Out** requires more resources (13 containers vs 2) but delivers superior stress performance
- **Monolith** simplest to deploy but higher stress latency
- **Single-Node Microservices** worst under stress (resource contention)

---

## 10. Appendix: Statistical Methodology

### Statistical Calculations
- **Mean (Î¼):** Average of all samples
- **Standard Deviation (Ïƒ):** Measure of dispersion from mean
  - **Method:** Sample standard deviation with Bessel's correction (ddof=1)
  - Formula: âˆš[Î£(xáµ¢ - Î¼)Â² / (n-1)]
  - Using n-1 provides unbiased estimate for population variance
- **95% Confidence Interval (CI):** Î¼ Â± (tâ‚€.â‚€â‚‚â‚… Ã— Ïƒ/âˆšn), where tâ‚€.â‚€â‚‚â‚… â‰ˆ 2.262 for n=10 (df=9)
  - Indicates range where true population mean likely falls with 95% confidence
  - Narrower CI indicates more precise estimate
- **Coefficient of Variation (CV):** (Ïƒ/Î¼) Ã— 100% - measure of relative variability

### Quality Assurance
- âœ… All 10 baseline runs verified: complete data from JSON exports (2025-12-31 r01-r10)
- âœ… All 10 stress runs verified: complete data from JSON exports (2025-12-31 r01-r10)
- âœ… Zero error rate confirmed across all runs (http_req_failed = 0%)
- âœ… All runs met SLO thresholds (p95 < 2.8s, p99 < 5.5s)
- âœ… Sample standard deviation (ddof=1) used consistently for all calculations
- âœ… 10 repetitions improve reliability and enable inferential statistics

### Limitations & Future Work
- **Single-Host Scale-Out:** Results specific to single-machine Docker Compose environment
  - Not representative of true distributed multi-host deployment
  - Fault tolerance limited to single-container failure (not host-level HA)
  - Network latency characteristics differ from multi-host configurations
- **Resource Monitoring:** CPU, memory, database metrics per replica not captured in this test cycle
  - Cannot determine actual resource utilization per replica
  - Bottleneck identification requires profiling tools
- **Container Limits:** No explicit CPU/memory limits set; actual resource usage not measured per replica
  - Replicas may compete unfairly for resources
  - Production deployment should set resource quotas
- **Load Generator Co-location:** k6 running on same host may impact results at high load
  - Future work: run k6 from separate machine for isolation
- **Network Latency:** Nginx routing overhead not isolated; future testing should measure per-hop latency
  - Baseline throughput lower than single-node (21.88 vs 27.58 req/s) warrants further investigation
- **Multi-Host Distribution:** True distributed deployment (across multiple physical hosts) not tested
  - Production readiness requires validation on multi-host clusters (Kubernetes, Docker Swarm)
- **Replica Count Optimization:** Testing with 4-5 replicas recommended to find optimal scaling point
- **Upper Load Limit:** Testing stops at 75 VUs; saturation point not identified (recommend 100-150 VUs)

### Data Collection
- **Test Runs (10 per scenario):**
  - 2025-12-31: r01 through r10 (all 10 runs)
  
- **Data Format:**
  - summary-export.json contains complete metrics in JSON format
  - All statistics calculated from summary-export.json for consistency

### File Locations
- Base path: `test-results/2025-12-31-[run]/microservices-scaled/[scenario]/`
- Detailed metrics: `summary-export.json` (JSON format)
- Quick summary: `summary.txt` (text format, p95/p99 only)
- Raw metrics: `metrics.csv` (CSV format)

---

**End of Report**
